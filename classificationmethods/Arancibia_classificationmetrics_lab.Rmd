---
title: "Classification Model Metrics"
author: "Ben Arancibia"
date: "May 2, 2015"
output: pdf_document
---

1) Read in the csv

```{r}
data <- read.csv("/users/bcarancibia/CUNY_IS_621/classificationmethods/classificationoutput.csv")
```

There are three key columns:
* class: the actual class of the observation
* Scored.Class: the predicted class of the observation
* Scored.Probability: the predicted probability of success for the observation

2) Use the table() function to get the raw confusion matrix.

```{r}

confusion.matrix <- table(data$class, data$Scored.Labels)

(119+27) / 181

```

The columns represent the actual class and the rows represent the predicted class.

What the confusion matrix is, is an easy to quickly show two types of errors. The two types of errors are those errors which are false negatives or false positives. The overall error percentage is quite low, around 80% was scored correctly, but the errors for each predicted class are different. For example, the number of actual scored 0s is 149, but the predicted 0s is only 119, error rate of 20%. For 1s, it is an error rate of 16.6% (5/30).

3) Write a function that takes the dataset as a dataframe with actual and predicted classifications identified. Return the accuracy of the predictions.

$Accuracy = (TP+TN)/(TP+FP+TN+FN)$

```{r}
#tp = true positive 0
#tn = true negative 1
#fp = false positve
#fn = false negative

TP <- 119
TN <- 27
FP <- 5
FN <- 30

accuracy <- (TP+TN)/(TP+FP+TN+FN)
accuracy
```

4) Write a function that takes the dataset as a dataframe to get the error rate

```{r}
error <- (FP+FN)/(TP+FP+TN+FN)
error

accuracy + error
```

5) Precision function

```{r}

precision <- TP/(TP+FP)
precision
```

6) Recall of the predictions also know as sensitivity

```{r}
recall <- TP/(TP+FN)
recall
```

7) Specificity 

```{r}
specificity <- TN/(TN+FP)
specificity
```

8) F1 score of the predictions

```{r}
F1 <- 2*((precision*recall)/(precision+recall))
F1
```

9)
The bounds of an F1 score will always be between 0 and 1. 

0 < a < 1
0 < b < 1

then 

ab < a

because it is a combination of precesion and recall which are already ratios. As a result, the ratios of precision and recall are between 0 and 1 and are tied together. Thus, F1 cannot go above. see below to see two highest possible scores.

```{r}
.99999999999999999 * 1
```


